  GNU nano 7.2                                            dev.env                                                     # ===== Environment meta =====
# ===== Environment meta =====
ENV_NAME=dev

# ===== Repo + data roots =====
REPO_ROOT=/opt/sharpe10/S10-INFRA

# Host data paths for Kafka/ZooKeeper (bind mounts)
DATA_ROOT=/opt/sharpe10/data
ZK_DATA=${DATA_ROOT}/zookeeper
KAFKA_DATA=${DATA_ROOT}/kafka

# ===== Canonical hostnames (you already mapped these in /etc/hosts) =====
SERVER1_HOST=server1    # ClickHouse bare metal
SERVER2_HOST=server2    # Prometheus/Grafana (your typical setup)
SERVER3_HOST=server3    # Kafka/ZooKeeper/Connect

# ===== ClickHouse (server1) =====
CH_HOST=${SERVER1_HOST}
CH_PORT=9000
CH_HTTP_PORT=8123
CH_USER=default
CH_PASSWORD=            # leave empty if no password
CH_DB=database1

# ===== Kafka (server3) =====
# Kafka exposes two listeners in your stack:
# - internal (Docker overlay): kafka:9092
# - external (host/other servers): server3:29092
KAFKA_BROKERS_INTERNAL=kafka:9092
KAFKA_BROKERS_EXTERNAL=${SERVER3_HOST}:29092
# Default for host-side scripts; change to INTERNAL when used inside containers
KAFKA_BROKERS=${KAFKA_BROKERS_EXTERNAL}

# ZooKeeper (adjust to INTERNAL if the caller is inside Docker)
ZK_CONNECT=${SERVER3_HOST}:2181
ZK_CONNECT_INTERNAL=zookeeper:2181

# Kafka Connect REST (Connect usually runs on server3)
CONNECT_HOST=${SERVER3_HOST}
CONNECT_PORT=8083
CONNECT_URL=http://${CONNECT_HOST}:${CONNECT_PORT}

# ===== ClickHouse Sink Connector (templated JSON) =====
CONNECTOR_NAME=clickhouse-sink
CONNECT_TASKS_MAX=4
CONNECT_TOPICS=docker_topic_1
CONNECT_TOPIC2TABLE=docker_topic_1=production_test_table_1
CONNECT_AUTO_CREATE=false
CONNECT_BATCH_SIZE=50000
CONNECT_LINGER_MS=500

# Consumer overrides used by the sink
KAFKA_FETCH_MIN_BYTES=52428800
KAFKA_FETCH_MAX_BYTES=524288000
KAFKA_FETCH_MAX_WAIT_MS=500
KAFKA_MAX_PARTITION_FETCH=52428800
KAFKA_MAX_POLL_RECORDS=100000


# ===== Common paths (adjust to your actual mounts) =====
BACKUP_ROOT=/mnt/backup
FREEZE_ROOT=/var/lib/clickhouse/shadow
LOG_DIR=/var/log/sharpe10

# ===== Monitoring (server3) =====
# IPs used by Prometheus scrapes
SERVER1_IP=10.0.0.208
SERVER2_IP=10.0.0.225
SERVER3_IP=10.0.0.210

# Ports
PROMETHEUS_PORT=9090
ALERTMANAGER_PORT=9093
GRAFANA_PORT=3000
NODE_EXPORTER_PORT=9100
KAFKA_EXPORTER_PORT=9308

# Bind mounts
PROM_ROOT=/opt/prometheus
ALERTM_ROOT=/opt/alertmanager
GRAFANA_ROOT=/opt/grafana

# Prometheus -> Alertmanager target (service name works inside Swarm overlay)
ALERTMANAGER_HOST=alertmanager

# Kafka Exporter (talks to your external listener on server3)
KAFKA_BROKER_ADDR=10.0.0.210:29092
KAFKA_VERSION=3.6.0

# Alertmanager email
ALERT_SMTP_HOSTPORT=smtp.gmail.com:587
ALERT_SMTP_FROM="Sharpe_10 Alerts <jmorrison@sharpe10.com>"
ALERT_SMTP_USERNAME=jmorrison@sharpe10.com
ALERT_SMTP_REQUIRE_TLS=true
ALERT_DEFAULT_RECEIVER=team-email
ALERT_EMAIL_TO=jmorrison@sharpe10.com
ALERT_GROUP_WAIT=30s
ALERT_GROUP_INTERVAL=5m
ALERT_REPEAT_INTERVAL=12h

# ===== Validation =====
# Kafka target (already used elsewhere)
# KAFKA_BROKER_ADDR=10.0.0.210:29092

# Topic & table mapping
VALIDATION_TOPIC=docker_topic_1
# Prefer this explicit table, OR leave it unset and rely on CONNECT_TOPIC2TABLE="topic=table"
# VALIDATION_CH_TABLE=production_test_table_1

# Batch/behavior
VALIDATION_BATCH_SIZE=10000
VALIDATION_COMMIT=0        # set to 1 to commit offsets after run
VALIDATION_USE_LOCK=0      # set to 1 to install deps from requirements.lock

# Output files (written where you run the script, unless absolute paths)
VALIDATION_SUMMARY=summary.json
VALIDATION_DETAILS=details.json
VALIDATION_BAD_ROWS=bad_rows.json
VALIDATION_CH_QUERY_LOG=ch_query_windows.json

# Where the Python validator loads SMTP (email) settings from
VALIDATION_DOTENV=/etc/sharpe10/validation.env